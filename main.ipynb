{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.15k/1.15k [00:00<?, ?B/s]\n",
      "vocab.json: 100%|██████████| 127k/127k [00:00<00:00, 243kB/s]\n",
      "merges.txt: 100%|██████████| 62.9k/62.9k [00:00<00:00, 271kB/s]\n",
      "added_tokens.json: 100%|██████████| 16.0/16.0 [00:00<00:00, 16.0kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 772/772 [00:00<00:00, 759kB/s]\n",
      "tokenizer.json: 100%|██████████| 310k/310k [00:00<00:00, 435kB/s]\n",
      "config.json: 100%|██████████| 1.57k/1.57k [00:00<?, ?B/s]\n",
      "pytorch_model.bin: 100%|██████████| 730M/730M [03:28<00:00, 3.50MB/s] \n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "generation_config.json: 100%|██████████| 347/347 [00:00<00:00, 49.6kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Nice to meet you, my name is john. What do you do for a living?</s>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making an utterance\n",
    "utterance = \"My name is gold, I like football and coding\"\n",
    "#tokenize the utterance\n",
    "inputs = tokenizer(utterance, return_tensors=\"pt\")\n",
    "#generate model results\n",
    "result = model.generate(**inputs)\n",
    "# result\n",
    "tokenizer.decode(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlenderBot: <s> Hi, nice to meet you. My name is samantha. How are you?</s>\n",
      "BlenderBot: <s> I'm in my early twenties. I've been through a lot in my life.</s>\n",
      "BlenderBot: <s> I live in the United States, in the southeastern part of the united states.</s>\n",
      "BlenderBot: <s> I live in Virginia, but I am moving to Georgia in a few months.</s>\n"
     ]
    }
   ],
   "source": [
    "utterance = \"\"\n",
    "while True:\n",
    "    utterance = input(\"You: \")\n",
    "    if utterance.lower() == \"exit\":\n",
    "        break\n",
    "    inputs = tokenizer(utterance, return_tensors=\"pt\")\n",
    "    result = model.generate(**inputs)\n",
    "    print(\"BlenderBot:\", tokenizer.decode(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation history\n",
    "history_conversation = []\n",
    "\n",
    "def get_response(input_text):\n",
    "    global history_conversation\n",
    "    # Append user input to history\n",
    "    history_conversation.append(input_text)\n",
    "    history = history_conversation[-5:]\n",
    "    # Tokenize the input with history\n",
    "    inputs = tokenizer(\"\\n\".join(history), return_tensors='pt', truncation=True, max_length=128)\n",
    "    # Generate response\n",
    "    outputs = model.generate(**inputs)\n",
    "    # Decode and add to history\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    history_conversation.append(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm doing well, thank you. How are you this fine evening? Do you have any plans?\n",
      " I'm good, thanks for asking. I'm going to watch a movie. What about you?\n",
      " I like to go to the movies. What kind of movies do you like to watch? \n"
     ]
    }
   ],
   "source": [
    "print(get_response(\"Hi, how are you?\"))\n",
    "print(get_response(\"What can you do?\"))\n",
    "print(get_response(\"What can you do?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
